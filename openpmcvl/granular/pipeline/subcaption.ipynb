{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import re\n",
    "import pickle\n",
    "import json\n",
    "from typing import Any, Dict, List\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "\n",
    "from openpmcvl.granular.pipeline.utils import load_dataset, save_jsonl\n",
    "\n",
    "PMC_ROOT = \"set this directory\"\n",
    "\n",
    "# Make sure .env file containt OPENAI_API_KEY\n",
    "load_dotenv()\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Subcaption Extraction**\n",
    "\n",
    "Extracts subfigure captions from figure captions using OpenAI's GPT-4o Batch API.\n",
    "\n",
    "## Pipeline\n",
    "1. Input: JSONL with metadata (captions + IDs)\n",
    "2. Generate batch API requests (50k limit)\n",
    "3. Submit to OpenAI batch processing\n",
    "4. Get results as structured subcaptions\n",
    "5. Save results to JSONL file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"\"\"\n",
    "Subfigure labels are letters referring to individual subfigures within a larger figure.\n",
    "This is a caption: \"%s\"\n",
    "Check if the caption contains explicit subfigure label. \n",
    "If not, output \"NO\" and end the generation. \n",
    "If yes, output \"YES\", then generate the subcaption of the subfigures according to the caption. \n",
    "The output should use the template:\n",
    "    YES\n",
    "    Subfigure-A: ...\n",
    "    Subfigure-B: ...\n",
    "    ...\n",
    "The label should be removed from subcaption.\n",
    "\"\"\".strip()\n",
    "\n",
    "caption = \"Try sample caption!\"\n",
    "\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-2024-08-06\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": PROMPT % caption},\n",
    "    ],\n",
    "    temperature=0,\n",
    "    max_tokens=500,\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_api_request(custom_id, system_content, user_content):\n",
    "    \"\"\"Generate a single API request in the required format.\"\"\"\n",
    "    return {\n",
    "        \"custom_id\": custom_id,\n",
    "        \"method\": \"POST\",\n",
    "        \"url\": \"/v1/chat/completions\",\n",
    "        \"body\": {\n",
    "            \"model\": \"gpt-4o-2024-08-06\",\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": system_content},\n",
    "                {\"role\": \"user\", \"content\": user_content},\n",
    "            ],\n",
    "            \"temperature\": 0,\n",
    "            \"max_tokens\": 2000,\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "def create_prompt(caption):\n",
    "    \"\"\"Create the prompt template with the given caption.\"\"\"\n",
    "    return PROMPT.strip() % caption\n",
    "\n",
    "\n",
    "def generate_jsonl(dataset, requests_file):\n",
    "    \"\"\"Generate JSONL file with API requests.\n",
    "    \n",
    "    Args:\n",
    "        dataset: List of metadata containing captions and IDs\n",
    "        requests_file: Path to output requests JSONL file\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    \n",
    "    # Open output file and write requests line by line\n",
    "    with open(requests_file, \"w\") as f:\n",
    "        for data in dataset:\n",
    "            count += 1\n",
    "            \n",
    "            # Skip first 50k entries (already processed)\n",
    "            if count <= 50000:  # Batch API can handle at most 50k requests\n",
    "                continue\n",
    "                \n",
    "            # Only process captions under 400 words\n",
    "            if len(data[\"caption\"].split()) <= 400:\n",
    "\n",
    "                # Generate API request for this caption\n",
    "                request = generate_api_request(\n",
    "                    custom_id=f\"{data['id']}\",\n",
    "                    system_content=\"You are a helpful assistant.\",\n",
    "                    user_content=create_prompt(data[\"caption\"]),\n",
    "                )\n",
    "                \n",
    "                # Write request as JSON line\n",
    "                f.write(json.dumps(request) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the metadata dataset containing captions and IDs\n",
    "dataset = load_dataset(os.path.join(PMC_ROOT, \"meta.jsonl\"))\n",
    "\n",
    "# Define output path for API requests\n",
    "requests_file = os.path.join(PMC_ROOT, \"requests.jsonl\")\n",
    "\n",
    "# Generate JSONL file with API requests for each caption\n",
    "generate_jsonl(dataset, requests_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the requests file to OpenAI for batch processing\n",
    "batch_input_file = client.files.create(file=open(requests_file, \"rb\"), purpose=\"batch\")\n",
    "batch_input_file_id = batch_input_file.id\n",
    "\n",
    "# Create a batch job to process the requests\n",
    "# This will run for up to 24 hours and process 50k subcaptions\n",
    "client.batches.create(\n",
    "    input_file_id=batch_input_file_id,\n",
    "    endpoint=\"/v1/chat/completions\",\n",
    "    completion_window=\"24h\", \n",
    "    metadata={\"description\": \"50k subcaptions\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note you have to run this separately for each submitted batch\n",
    "# Check status of first batch job\n",
    "print(client.batches.retrieve(\"batch_xxxxx\"))\n",
    "\n",
    "# Get the output file content from the completed batch\n",
    "file_response = client.files.content(\"file-xxxxxx\")\n",
    "\n",
    "# Write the batch results to a JSONL file\n",
    "with open(f\"{PMC_ROOT}/subcaptions.jsonl\", \"w\") as f:\n",
    "    f.write(file_response.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
