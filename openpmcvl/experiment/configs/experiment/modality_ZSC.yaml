# @package _global_

defaults:
  - /datasets@datasets.test.medsam: MedSAM
  - /datasets/transforms@datasets.test.medsam.transform: biomedclip_vision_transform
  - /datasets/tokenizers@dataloader.test.collate_fn.batch_processors.text: BiomedCLIPTokenizer
  - /modules/encoders@task.encoders.text: BiomedCLIPText
  - /modules/encoders@task.encoders.rgb: BiomedCLIPVision
  - /modules/layers@task.postprocessors.norm_and_logit_scale.norm: L2Norm
  - /modules/layers@task.postprocessors.norm_and_logit_scale.logit_scale: LearnableLogitScaling
  - /eval_task@task.evaluation_tasks.classification.task: ZeroShotClassification
  - /datasets/tokenizers@task.evaluation_tasks.classification.task.tokenizer: BiomedCLIPTokenizer
  - /trainer/logger@trainer.logger.wandb: WandbLogger
  - override /task: ContrastivePretraining
  - _self_

seed: 0
job_type: eval

datasets:
  test:
    medsam:
      split: test
      transform:
        job_type: eval

dataloader:
  test:
    batch_size: 64
    num_workers: 4

task:
  postprocessors:
    norm_and_logit_scale:
      norm:
        dim: -1
      logit_scale:
        learnable: True
  modality_module_mapping:
    text:
      postprocessor_key: norm_and_logit_scale
    rgb:
      postprocessor_key: norm_and_logit_scale
  evaluation_tasks:
    classification:
      task:
        task_specs:
          - top_k: [1]
            query_modality: rgb
      run_on_validation: false
      run_on_test: true
  compute_validation_loss: False
  compute_test_loss: False

trainer:
  precision: 16-mixed
  deterministic: False
  benchmark: True
  sync_batchnorm: False # set to True if using DDP with batchnorm
  log_every_n_steps: 100

tags:
  - ${experiment_name}
  - zeroshot
  - classification
