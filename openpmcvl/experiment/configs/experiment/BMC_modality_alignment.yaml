# @package _global_

defaults:
  - /datasets@datasets.train.pmcvl: PMCOA_2
  - /datasets/transforms@datasets.train.pmcvl.transform: biomedclip_vision_transform
  - /datasets@datasets.val.pmcvl: PMCOA_2
  - /datasets/transforms@datasets.val.pmcvl.transform: biomedclip_vision_transform
  - /datasets@datasets.test.pmcvl: PMCOA_2
  - /datasets/transforms@datasets.test.pmcvl.transform: biomedclip_vision_transform
  - /datasets/tokenizers@dataloader.train.collate_fn.batch_processors.text: BiomedCLIPTokenizer
  - /datasets/tokenizers@dataloader.val.collate_fn.batch_processors.text: BiomedCLIPTokenizer
  - /datasets/tokenizers@dataloader.test.collate_fn.batch_processors.text: BiomedCLIPTokenizer
  - /modules/encoders@task.encoders.text: BiomedCLIPText
  - /modules/encoders@task.encoders.rgb: BiomedCLIPVision
  - /modules/encoders@task.encoders.endoscopy: BiomedCLIPVisionModality
  - /modules/encoders@task.encoders.mr: BiomedCLIPVisionModality
  - /modules/encoders@task.encoders.oct: BiomedCLIPVisionModality
  - /modules/encoders@task.encoders.us: BiomedCLIPVisionModality
  - /modules/encoders@task.encoders.microscopy: BiomedCLIPVisionModality
  - /modules/encoders@task.encoders.mammography: BiomedCLIPVisionModality
  - /modules/encoders@task.encoders.pet: BiomedCLIPVisionModality
  - /modules/encoders@task.encoders.fundus: BiomedCLIPVisionModality
  - /modules/encoders@task.encoders.dermoscopy: BiomedCLIPVisionModality
  - /modules/encoders@task.encoders.ct: BiomedCLIPVisionModality
  - /modules/encoders@task.encoders.xray: BiomedCLIPVisionModality
  - /modules/layers@task.postprocessors.norm_and_logit_scale.norm: L2Norm
  - /modules/layers@task.postprocessors.norm_and_logit_scale.logit_scale: LearnableLogitScaling
  - /modules/losses@task.loss: CLIPLoss
  - /modules/optimizers@task.optimizer: AdamW
  - /modules/lr_schedulers@task.lr_scheduler.scheduler: CosineAnnealingWarmupLR
  - /eval_task@task.evaluation_tasks.retrieval.task: ZeroShotCrossModalRetrieval
  - /trainer/callbacks@trainer.callbacks.lr_monitor: LearningRateMonitor
  - /trainer/callbacks@trainer.callbacks.model_checkpoint: ModelCheckpoint
  - /trainer/callbacks@trainer.callbacks.early_stopping: EarlyStopping
  - /trainer/callbacks@trainer.callbacks.model_summary: ModelSummary
  - /trainer/logger@trainer.logger.wandb: WandbLogger
  - override /task: ContrastivePretrainingModality
  - _self_

seed: 0

datasets:
  train:
    pmcvl:
      split: test
  val:
    pmcvl:
      split: test
      transform:
        job_type: eval
  test:
    pmcvl:
      split: test
      transform:
        job_type: eval

dataloader:
  train:
    batch_size: 256
    num_workers: 4
  val:
    batch_size: 32
    num_workers: 4
  test:
    num_workers: 4

task:
  encoders:
    endoscopy:
      modality: endoscopy
    mr:
      modality: mr
    oct:
      modality: oct
    us:
      modality: us
    microscopy:
      modality: microscopy
    mammography:
      modality: mammography
    pet:
      modality: pet
    fundus:
      modality: fundus
    dermoscopy:
      modality: dermoscopy
    ct:
      modality: ct
    xray:
      modality: xray
  postprocessors:
    norm_and_logit_scale:
      norm:
        dim: -1
      logit_scale:
        learnable: True
  modality_module_mapping:
    text:
      postprocessor_key: norm_and_logit_scale
    rgb:
      postprocessor_key: norm_and_logit_scale
    Endoscopy:
      postprocessor_key: norm_and_logit_scale
    mr:
      postprocessor_key: norm_and_logit_scale
    oct:
      postprocessor_key: norm_and_logit_scale
    us:
      postprocessor_key: norm_and_logit_scale
    microscopy:
      postprocessor_key: norm_and_logit_scale
    mammography:
      postprocessor_key: norm_and_logit_scale
    pet:
      postprocessor_key: norm_and_logit_scale
    fundus:
      postprocessor_key: norm_and_logit_scale
    dermoscopy:
      postprocessor_key: norm_and_logit_scale
    ct:
      postprocessor_key: norm_and_logit_scale
    xray:
      postprocessor_key: norm_and_logit_scale
  optimizer:
    betas:
    - 0.9
    - 0.98
    lr: 5.0e-4
    weight_decay: 0.2
    eps: 1.0e-6
  lr_scheduler:
    scheduler:
      t_max: 104_671 # make sure to change this if max_epochs or accumulate_grad_batches is changed
      warmup_length: 2000
    extras:
      interval: step
  loss:
    gather_with_grad: True
    local_loss: True
  evaluation_tasks:
    retrieval:
      task:
        task_specs:
          - query_modality: text
            target_modality: rgb
            top_k: [1, 5, 10]
          - query_modality: rgb
            target_modality: text
            top_k: [1, 5, 10]
      run_on_validation: false
      run_on_test: true

trainer:
  max_epochs: 32
  precision: bf16-mixed
  deterministic: False
  benchmark: True
  sync_batchnorm: False # set to True if using DDP with batchnorm
  log_every_n_steps: 100
  accumulate_grad_batches: 4
  check_val_every_n_epoch: 1
  callbacks:
    model_checkpoint:
      monitor: val/loss
      save_top_k: -1
      save_last: True
      every_n_epochs: 1
      dirpath: /checkpoint/${oc.env:USER}/${oc.env:SLURM_JOB_ID} # only works on Vector SLURM environment
      verbose: True
    early_stopping:
      monitor: val/loss
      patience: 5
      mode: min
      verbose: True
    model_summary:
      max_depth: 2

tags:
  - ${experiment_name}
  - contrastive pretraining
  - rgb
  - text
  - clip
  - pmcvl
  - openpmcvl
