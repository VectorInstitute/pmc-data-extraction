#!/bin/bash

#SBATCH --job-name=modality_specific_5_encoder
#SBATCH --mem=0
#SBATCH --qos=a40_arashaf_multimodal
#SBATCH --partition=a40
#SBATCH --gres=gpu:4
#SBATCH --cpus-per-task=8
#SBATCH --ntasks-per-node=4
#SBATCH --nodes=1
#SBATCH --time=72:00:00
#SBATCH --export=ALL
#SBATCH --output=outputs/slurm-%j-%N.out
#SBATCH --open-mode=append

# load virtual environment
source /projects/DeepLesion/envs/negin/pmc-data-extraction_env/env_3/bin/activate

cd /projects/DeepLesion/projects/pmc-data-extraction
export PYTHONPATH="."

# export NCCL_IB_DISABLE=1  # disable InfiniBand (the Vector cluster does not have it)
# export NCCL_DEBUG=WARN
# export NCCL_DEBUG_SUBSYS=WARN
export TORCH_NCCL_ASYNC_ERROR_HANDLING=3
# export CUDA_LAUNCH_BLOCKING=1
# export TORCH_DISTRIBUTED_DEBUG=DETAIL
export HYDRA_FULL_ERROR=1
# export OMP_NUM_THREADS=12

export MASTER_ADDR=$(hostname)
export MASTER_PORT=45678

nvidia-smi
echo SLURM_ARRAY_JOB_ID=${SLURM_ARRAY_JOB_ID}
echo SLURM_JOBID=${SLURM_JOBID}

# “srun” executes the script <ntasks-per-node * nodes> times
srun --export=ALL -N $SLURM_JOB_NUM_NODES --cpu_bind=v --accel-bind=gn \
    mmlearn_run \
    'hydra.searchpath=[pkg://openpmcvl.experiment.configs]' \
    +experiment=modality_specific_5_encoder \
    experiment_name=modality_specific_5_encoder \
    datasets/tokenizers@dataloader.train.collate_fn.batch_processors.text=BiomedCLIPTokenizerOG \
    datasets/tokenizers@dataloader.val.collate_fn.batch_processors.text=BiomedCLIPTokenizerOG \
    datasets/tokenizers@dataloader.test.collate_fn.batch_processors.text=BiomedCLIPTokenizerOG \
    dataloader.train.batch_size=64 \
    dataloader.val.batch_size=32 \
    dataloader.train.num_workers=4 \
    dataloader.val.num_workers=4 \
    task.encoders.text.pretrained=False \
    task.encoders.d3.pretrained=False \
    task.encoders.dm.pretrained=False \
    task.encoders.ds.pretrained=False \
    task.encoders.dr.pretrained=False \
    task.encoders.dv.pretrained=False \
    task.lr_scheduler.scheduler.t_max=109120 \
    task.lr_scheduler.scheduler.warmup_length=2000 \
    ~trainer.callbacks.early_stopping
